{"name":"Wesley Tansey's implementation of a Reinforcement Learning agent for Tic-Tac-Toe","tagline":"Fork of a Reinforcement Learning agent for Tic-Tac-Toe. Implements the example from Chapter 1 of Sutton and Barto.","body":"# Reinforcement Learning in 3x3 Tic-Tac-Toe, *learning by random self-playing*\r\n> Implementation in Python (2 or 3), forked from [tansey/rl-tictactoe](https://github.com/tansey/rl-tictactoe).\r\n\r\nA quick Python implementation of the **3x3** *Tic-Tac-Toe* **value function learning agent**, as described in Chapter 1 of\r\n[*\"Reinforcement Learning: An Introduction\"* by Sutton and Barto](http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html).\r\n\r\n----\r\n\r\n### Usage of this program\r\nThis implementation is simply [one Python file (``tictactoe.py``)](./tictactoe.py):\r\n\r\n```bash\r\n# Run this program and keep its output\r\npython2 tictactoe.py | tee ./tictactoe.log\r\n```\r\n\r\n### Example\r\nSee the figure below for an example of what is achieved with this rather simple implementation:\r\n![Main example](selfplay_random_-1loss.png \"Example of selfplay vs random\")\r\n\r\nNumerical results are also available in the (long) [results.csv](./results.csv) CSV file.\r\n\r\n### Limitation\r\n> Only 3 by 3 (3x3) Tic-Tac-Toe is implemented.\r\n> [See this other project for a higher-dimensional Tic-Tac-Toe](http://naereen.github.io/Pengkun--Master-Thesis--2015/).\r\n\r\n----\r\n\r\n### Explanations\r\nThe agent contains a lookup table that maps states to values, where initial values are 1 for a win, 0 for a draw or loss, and 0.5 otherwise.\r\nAt every move, the agent chooses either the maximum-value move (greedy) or, with some probability epsilon, a random move (exploratory); by default ``epsilon=0.1``.\r\n\r\nThe agent updates its value function (the lookup table) after every greedy move, following the equation:\r\n\r\n```\r\nV(s) <- V(s) + alpha * ( V(s') - V(s) )\r\n```\r\n\r\n#### Why?\r\nThis particular implementation addresses the question posed in Exercise 1.1:\r\n\r\n> *What would happen if the RL agent taught itself via self-play?*\r\n\r\nThe result is that the agent learns only how to maximize its own potential payoff,\r\nwithout consideration for whether it is playing to a win or a draw.\r\nEven more to the point, the agent learns a myopic strategy where it basically has a single path that it wants to take to reach a winning state.\r\n\r\nIf the path is blocked by the opponent, the values will then usually all become 0.5 and the player is effectively moving randomly (so we did nothing clever in this case).\r\n\r\nNote that if you use a loss value of -1, then the agent learns to play the optimal strategy in the minimax sense.\r\n\r\n----\r\n\r\n### License and authors\r\n- Forked and cleaned up by [Lilian Besson (Naereen)](https://github.com/Naereen), 28/12/2015,\r\n- Created by [Wesley Tansey](https://github.com/tansey/), 1/21/2013,\r\n- Code released under the [MIT license](http://lbesson.mit-license.org).\r\n","google":"UA-38514290-17","note":"Don't delete this file! It's used internally to help with page regeneration."}